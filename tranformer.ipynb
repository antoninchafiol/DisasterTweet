{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "This notebook is used to test the classes used and implemented in the transformer_classes.py file\n",
    "\n",
    "Remember:\n",
    "We need to implement the following (found in \"Attention is all you need\" paper available at https://arxiv.org/pdf/1706.03762.pdf):\n",
    "\n",
    "- Transformer Architecture\n",
    "- Scalled Dot Product\n",
    "- Multi-Head Attention\n",
    "\n",
    "And all other things required like positionnal encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function Classes\n",
    "## I. Build Vocab from Glove & Embedding\n",
    "\n",
    "First we'll load and setup glove and our vocab to get the first \"brick\" for embedding\n",
    "\n",
    "For this test I'll load all glove pretrained weight and build the vocab\n",
    "\n",
    "#### 1st Version\n",
    "\n",
    "Using Glove and the same data cleaning method as the one in the main branch which let lots of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Main/global imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from textfn import *\n",
    "from classes import *\n",
    "from tranformer_classes import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loadDts('dataset/train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 50 # Renammed from embedding_dim\n",
    "glove_path = 'glove_pretrained/glove.6B.{}d.txt'.format(d_model)\n",
    "vocab_size = 10000\n",
    "max_seq_length = 20\n",
    "\n",
    "embedding_weights = np.zeros((vocab_size+2, d_model))\n",
    "word_to_index = {}\n",
    "index=0\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if index >= vocab_size-2:\n",
    "            break\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float16\")\n",
    "        embedding_weights[index] = vector\n",
    "        word_to_index[word] = index\n",
    "        index +=1 \n",
    "    f.close()\n",
    "embedding_weights[index+1] = np.ones(d_model)\n",
    "embedding_weights[index+2] = np.zeros(d_model)\n",
    "word_to_index['<unk>'] = index+1\n",
    "word_to_index['<pad>'] = index+2\n",
    "vocab_size+=2\n",
    "embedding_weights = torch.tensor(embedding_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TranformerGloveDataset(df, max_seq_length, word_to_index, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding_mask(input, padding_idx):\n",
    "    padding_mask = (input == padding_idx)\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. i. Embedding Handling\n",
    "\n",
    "As we built the vocab, we'll need to handle the embedded values of the words.\n",
    "\n",
    "#### Method 1:\n",
    "\n",
    "Using Glove's pretrained weights and freezing the layer in the class Embedder\n",
    "\n",
    "#### Method 2:\n",
    "\n",
    "Idem as method 1 but putting this direclty in the Encoder class using the embedding layer\n",
    "\n",
    "For now, using Method 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Positional Encoding\n",
    "\n",
    "Positional Encoding is a matrix which define the position of the word in the sentence.\n",
    "\n",
    "It's defined with:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "And\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "As stated on the original paper: \n",
    "\"The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed.\"\n",
    "\n",
    "\n",
    "So the dimensions of the PE matrix are the **sentence size** and **embedding size** or $d_{model}$\n",
    "\n",
    "##### Method 1\n",
    "\n",
    "Create a class that compute the positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example to check if positional encoder work\n",
    "# data = torch.randn(5, 10, 6)\n",
    "# pos_enc = PositionalEncoder(10, 6)\n",
    "# encoded_input = pos_enc(data)\n",
    "\n",
    "# print(encoded_input.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Multi-Head Attention & Scaled Dot-Product Attention \n",
    "\n",
    "Multi-Head Attention is the one of the \"main\" component of the transformer network.\n",
    "\n",
    "It's using a set of matrices which will be trained to handle a specific role in the network:\n",
    "- **Queries (Q)**: Relationship & Dependencies with tokens in sequence.\n",
    "- **Keys (K)**: Key information used to compare against when computing scores.\n",
    "- **Values (V)**: Weighted sum of the mechanism\n",
    "\n",
    "Those matrices are made/initialized from inputs' embeddings vector with the positional encoding.\n",
    "\n",
    "\n",
    "In the Multi-Head Attention, we split the embedding into multiple layers (or **heads**) where $N$ is the number of head. $d_k$ will be refering to the last dimension where $d_k = d_{model}/N$\n",
    "\n",
    "**Dropout**: As the original paper state: \"_We apply dropout to the sums of the embeddings and the positional encoding in both the encoder and decoder stacks. For the base model we use a rate of_ $P_{drop}=0.1$\"\n",
    "\n",
    "### III. i Scaled Dot-Product Attention\n",
    "\n",
    "As the original paper stated, this Attention is computed as: $$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Without forgetting that we can add both mask and dropout to this.\n",
    "\n",
    "#### Method 1 \n",
    "\n",
    "Make the Multi-Head Attention class and the Attention (Scaled Dot-Product Attention) as a function in it.\n",
    "\n",
    "#### Method 2 \n",
    "\n",
    "Idem as method 1, but putting the Attention in a splitted class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Feed-Forward Network\n",
    "\n",
    "The Feed-Forward \"layer\" serve the purpose of deepens the whole networks by using Linear layers.\n",
    "\n",
    "As stated in the original paper: \"_This consists of two linear transformations with a ReLU activation in between_\"\n",
    "\n",
    "The number set per default of $d_{ff}$ is stated in original paper: \"_[...] and the inner-layer has dimensionality_ $d_{ff} = 2048$\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Normalization \n",
    "\n",
    "Normalization is important for our network, to prevent our values to not change too much, so model can train faster and better.\n",
    "\n",
    "Original paper state that they're using Layer Normalization.\n",
    "To implement LN, we need to implement the following:\n",
    "$$LN(z;\\alpha,\\beta) = \\frac{z-\\mu}{\\sigma}\\odot \\alpha + \\beta$$\n",
    "\n",
    "This can be found into the \"Layer Normalization\" paper, Page 13, 15 & 16th formula [HERE](https://arxiv.org/pdf/1607.06450.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Main blocks and Architecture\n",
    "\n",
    "## I. Encoder/Decoder & Problem\n",
    "\n",
    "While both Encoder & Decoder Blocks differ a bit:\n",
    "- Encoder has \"only\" 1 Multi-Head Attention and 1 Feed Forward\n",
    "- Decoder has 2 Multi-Head Attention and 1 Feed Forward and receive Encoder output\n",
    "\n",
    "The common thing between the 2 is the skipped connections and the layers used.\n",
    "So no particular difficulties in implementing the blocks.\n",
    "\n",
    "With all previous classes implemented, we can make the parts classes by adding our embeddings/positional encoding and using either copy.deepcopy() or nn.ModuleList() to can create multiple independent blocks/module for our model to work with.\n",
    "\n",
    "Althought the current transformer is \"finished\", the reference used implemented it for sequence to sequence, but Disaster Tweet is a Sentiment Analysis task, so we need to change a few things to make a sentiment analysis task from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Changes for Sentiment Analysis\n",
    "\n",
    "### i. Transformer class\n",
    "\n",
    "As sentiment analysis is a Many-to-One setup, we don't need the Decoder part of the transformer.\n",
    "So we we'll create a new class for sentiment analysis without decoder, adapt the output, Linear layer and forward computation for a binary output.\n",
    "\n",
    "#### a. Different setups\n",
    "\n",
    "There's different setups related to NLP:\n",
    "- **Many-to-One**: Take a sequence and map it to two or more classes (Ex: Sentiment Analysis)\n",
    "- **Many-to-Many**: Both inputs and output are sequences (Ex: Machine Translation)\n",
    "- **One-to-Many**: Input is a single value and output a sequence (Ex: Image Captioning) \n",
    "\n",
    "In this case it's Many-to_one and we don't need Decoder blocks.\n",
    "\n",
    "#### b. Parameters\n",
    "\n",
    "Change the parameters to adapt to a vocab in entry + number of classes expected as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Global\n",
    "epochs = 100\n",
    "batch_size = 2048\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "split_seed = 42\n",
    "train_dev_split = 0.65\n",
    "\n",
    "# Model\n",
    "N = 1\n",
    "num_classes = 1\n",
    "heads = 1\n",
    "\n",
    "# Optimizer\n",
    "optim_lr = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data = torch.utils.data.random_split(data, [train_dev_split, 1-train_dev_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_data,   batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentAnalysisTransformer(vocab_size, max_seq_length, num_classes, d_model, N, heads, embedding_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt     = torch.optim.Adam(model.parameters(), lr=optim_lr)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "metric  = F1Score(task='binary').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 - TRAIN] - Loss: 1.8645762205123901 - Acc: 0.27329128980636597\n",
      "[Epoch 1 - TRAIN] - Loss: 1.0768431425094604 - Acc: 0.3853113353252411\n",
      "[Epoch 2 - TRAIN] - Loss: 0.7736183404922485 - Acc: 0.4600411355495453\n",
      "[Epoch 3 - TRAIN] - Loss: 0.7171636422475179 - Acc: 0.5240249037742615\n",
      "[Epoch 4 - TRAIN] - Loss: 0.7005357543627421 - Acc: 0.5613490343093872\n",
      "[Epoch 5 - TRAIN] - Loss: 0.6992269357045492 - Acc: 0.5308534502983093\n",
      "[Epoch 6 - TRAIN] - Loss: 0.6931167244911194 - Acc: 0.2062465250492096\n",
      "[Epoch 7 - TRAIN] - Loss: 0.6852424343427023 - Acc: 0.00768939359113574\n",
      "[Epoch 8 - TRAIN] - Loss: 0.6905475457509359 - Acc: 0.05803803354501724\n",
      "[Epoch 9 - TRAIN] - Loss: 0.6899590094884237 - Acc: 0.11740415543317795\n",
      "[Epoch 10 - TRAIN] - Loss: 0.6851421991984049 - Acc: 0.014471235685050488\n",
      "[Epoch 11 - TRAIN] - Loss: 0.6859024961789449 - Acc: 0.01727413758635521\n",
      "[Epoch 12 - TRAIN] - Loss: 0.6864095131556193 - Acc: 0.005471400450915098\n",
      "[Epoch 13 - TRAIN] - Loss: 0.685403068860372 - Acc: 0.01893981173634529\n",
      "[Epoch 14 - TRAIN] - Loss: 0.6835856239000956 - Acc: 0.02941860817372799\n",
      "[Epoch 15 - TRAIN] - Loss: 0.6801734964052836 - Acc: 0.04740311950445175\n",
      "[Epoch 16 - TRAIN] - Loss: 0.6747909784317017 - Acc: 0.19051778316497803\n",
      "[Epoch 17 - TRAIN] - Loss: 0.6666330297787985 - Acc: 0.39712971448898315\n",
      "[Epoch 18 - TRAIN] - Loss: 0.6735047896703085 - Acc: 0.4286589026451111\n",
      "[Epoch 19 - TRAIN] - Loss: 0.6440044244130453 - Acc: 0.28674983978271484\n",
      "[Epoch 20 - TRAIN] - Loss: 0.6191671888033549 - Acc: 0.528732419013977\n",
      "[Epoch 21 - TRAIN] - Loss: 0.6034046014149984 - Acc: 0.6220697164535522\n",
      "[Epoch 22 - TRAIN] - Loss: 0.5893622040748596 - Acc: 0.6350307464599609\n",
      "[Epoch 23 - TRAIN] - Loss: 0.5862141052881876 - Acc: 0.5815340876579285\n",
      "[Epoch 24 - TRAIN] - Loss: 0.5831218361854553 - Acc: 0.6584180593490601\n",
      "[Epoch 25 - TRAIN] - Loss: 0.5686262051264445 - Acc: 0.6732751727104187\n",
      "[Epoch 26 - TRAIN] - Loss: 0.5566034913063049 - Acc: 0.6764415502548218\n",
      "[Epoch 27 - TRAIN] - Loss: 0.5514172514279684 - Acc: 0.6710323691368103\n",
      "[Epoch 28 - TRAIN] - Loss: 0.5373095671335856 - Acc: 0.6796379089355469\n",
      "[Epoch 29 - TRAIN] - Loss: 0.5282443960507711 - Acc: 0.6811789274215698\n",
      "[Epoch 30 - TRAIN] - Loss: 0.5684751073519388 - Acc: 0.6566358804702759\n",
      "[Epoch 31 - TRAIN] - Loss: 0.5606771310170492 - Acc: 0.6222857236862183\n",
      "[Epoch 32 - TRAIN] - Loss: 0.549141506354014 - Acc: 0.7062730193138123\n",
      "[Epoch 33 - TRAIN] - Loss: 0.5444785952568054 - Acc: 0.6485157012939453\n",
      "[Epoch 34 - TRAIN] - Loss: 0.5309494336446127 - Acc: 0.6707944869995117\n",
      "[Epoch 35 - TRAIN] - Loss: 0.5297489960988363 - Acc: 0.700668215751648\n",
      "[Epoch 36 - TRAIN] - Loss: 0.5239174365997314 - Acc: 0.6766752004623413\n",
      "[Epoch 37 - TRAIN] - Loss: 0.5188654462496439 - Acc: 0.7044844031333923\n",
      "[Epoch 38 - TRAIN] - Loss: 0.5159839590390524 - Acc: 0.6904668211936951\n",
      "[Epoch 39 - TRAIN] - Loss: 0.5066012342770895 - Acc: 0.694729208946228\n",
      "[Epoch 40 - TRAIN] - Loss: 0.5183770060539246 - Acc: 0.7012194395065308\n",
      "[Epoch 41 - TRAIN] - Loss: 0.5229618350664774 - Acc: 0.6714579463005066\n",
      "[Epoch 42 - TRAIN] - Loss: 0.5167597333590189 - Acc: 0.6940991282463074\n",
      "[Epoch 43 - TRAIN] - Loss: 0.5122881730397543 - Acc: 0.7089647054672241\n",
      "[Epoch 44 - TRAIN] - Loss: 0.506865531206131 - Acc: 0.6791658401489258\n",
      "[Epoch 45 - TRAIN] - Loss: 0.5165455540021261 - Acc: 0.7118615508079529\n",
      "[Epoch 46 - TRAIN] - Loss: 0.5156210660934448 - Acc: 0.6835983991622925\n",
      "[Epoch 47 - TRAIN] - Loss: 0.5147999127705892 - Acc: 0.6898447871208191\n",
      "[Epoch 48 - TRAIN] - Loss: 0.5103881259759268 - Acc: 0.7113217115402222\n",
      "[Epoch 49 - TRAIN] - Loss: 0.4993590712547302 - Acc: 0.6925565600395203\n",
      "[Epoch 50 - TRAIN] - Loss: 0.5065590540568033 - Acc: 0.7131041884422302\n",
      "[Epoch 51 - TRAIN] - Loss: 0.49899906913439435 - Acc: 0.7154929637908936\n",
      "[Epoch 52 - TRAIN] - Loss: 0.5081250667572021 - Acc: 0.6920739412307739\n",
      "[Epoch 53 - TRAIN] - Loss: 0.5038612882296244 - Acc: 0.6978599429130554\n",
      "[Epoch 54 - TRAIN] - Loss: 0.4921241005261739 - Acc: 0.716147780418396\n",
      "[Epoch 55 - TRAIN] - Loss: 0.4987462063630422 - Acc: 0.7070207595825195\n",
      "[Epoch 56 - TRAIN] - Loss: 0.4971090853214264 - Acc: 0.7009996175765991\n",
      "[Epoch 57 - TRAIN] - Loss: 0.5020153323809305 - Acc: 0.7112812995910645\n",
      "[Epoch 58 - TRAIN] - Loss: 0.4997842013835907 - Acc: 0.7144473791122437\n",
      "[Epoch 59 - TRAIN] - Loss: 0.5000293056170145 - Acc: 0.7035632133483887\n",
      "[Epoch 60 - TRAIN] - Loss: 0.491363267103831 - Acc: 0.7155978679656982\n",
      "[Epoch 61 - TRAIN] - Loss: 0.49559565385182697 - Acc: 0.7127036452293396\n",
      "[Epoch 62 - TRAIN] - Loss: 0.5008145074049631 - Acc: 0.7025119066238403\n",
      "[Epoch 63 - TRAIN] - Loss: 0.4923190971215566 - Acc: 0.7102867960929871\n",
      "[Epoch 64 - TRAIN] - Loss: 0.49030277132987976 - Acc: 0.7134791612625122\n",
      "[Epoch 65 - TRAIN] - Loss: 0.5033688644568125 - Acc: 0.7004005312919617\n",
      "[Epoch 66 - TRAIN] - Loss: 0.49565736452738446 - Acc: 0.7196688652038574\n",
      "[Epoch 67 - TRAIN] - Loss: 0.5011300543944041 - Acc: 0.7097005248069763\n",
      "[Epoch 68 - TRAIN] - Loss: 0.5032745699087778 - Acc: 0.7085225582122803\n",
      "[Epoch 69 - TRAIN] - Loss: 0.5138760705788931 - Acc: 0.7032550573348999\n",
      "[Epoch 70 - TRAIN] - Loss: 0.5030871232350668 - Acc: 0.6897322535514832\n",
      "[Epoch 71 - TRAIN] - Loss: 0.5079128742218018 - Acc: 0.7060942053794861\n",
      "[Epoch 72 - TRAIN] - Loss: 0.5033615231513977 - Acc: 0.7176926136016846\n",
      "[Epoch 73 - TRAIN] - Loss: 0.49575554331143695 - Acc: 0.7072890996932983\n",
      "[Epoch 74 - TRAIN] - Loss: 0.4926881194114685 - Acc: 0.7205148935317993\n",
      "[Epoch 75 - TRAIN] - Loss: 0.4963552455107371 - Acc: 0.7067989110946655\n",
      "[Epoch 76 - TRAIN] - Loss: 0.4885261356830597 - Acc: 0.7021574974060059\n",
      "[Epoch 77 - TRAIN] - Loss: 0.5029097497463226 - Acc: 0.6967000365257263\n",
      "[Epoch 78 - TRAIN] - Loss: 0.5025950769583384 - Acc: 0.7119877338409424\n",
      "[Epoch 79 - TRAIN] - Loss: 0.5019002556800842 - Acc: 0.6865910291671753\n",
      "[Epoch 80 - TRAIN] - Loss: 0.5055247147878011 - Acc: 0.6947324275970459\n",
      "[Epoch 81 - TRAIN] - Loss: 0.4978385269641876 - Acc: 0.713108241558075\n",
      "[Epoch 82 - TRAIN] - Loss: 0.5076572497685751 - Acc: 0.6844295263290405\n",
      "[Epoch 83 - TRAIN] - Loss: 0.5077322324117025 - Acc: 0.7226750254631042\n",
      "[Epoch 84 - TRAIN] - Loss: 0.5052238007386526 - Acc: 0.6906256675720215\n",
      "[Epoch 85 - TRAIN] - Loss: 0.4939187169075012 - Acc: 0.7014812231063843\n",
      "[Epoch 86 - TRAIN] - Loss: 0.49982080856959027 - Acc: 0.7121062278747559\n",
      "[Epoch 87 - TRAIN] - Loss: 0.49456114570299786 - Acc: 0.692285418510437\n",
      "[Epoch 88 - TRAIN] - Loss: 0.5036445458730062 - Acc: 0.7059199213981628\n",
      "[Epoch 89 - TRAIN] - Loss: 0.49416589736938477 - Acc: 0.7105306386947632\n",
      "[Epoch 90 - TRAIN] - Loss: 0.50477734208107 - Acc: 0.7019392848014832\n",
      "[Epoch 91 - TRAIN] - Loss: 0.4957114557425181 - Acc: 0.7178744077682495\n",
      "[Epoch 92 - TRAIN] - Loss: 0.5003970861434937 - Acc: 0.6864709854125977\n",
      "[Epoch 93 - TRAIN] - Loss: 0.48879313468933105 - Acc: 0.719943642616272\n",
      "[Epoch 94 - TRAIN] - Loss: 0.4886725942293803 - Acc: 0.7153880596160889\n",
      "[Epoch 95 - TRAIN] - Loss: 0.4945809443791707 - Acc: 0.708093523979187\n",
      "[Epoch 96 - TRAIN] - Loss: 0.4942381978034973 - Acc: 0.7099749445915222\n",
      "[Epoch 97 - TRAIN] - Loss: 0.4942012429237366 - Acc: 0.7045595645904541\n",
      "[Epoch 98 - TRAIN] - Loss: 0.48805542786916095 - Acc: 0.7025184631347656\n",
      "[Epoch 99 - TRAIN] - Loss: 0.500085304180781 - Acc: 0.7102071642875671\n"
     ]
    }
   ],
   "source": [
    "t_size = len(train_loader)\n",
    "d_size = len(dev_loader)\n",
    "p = True\n",
    "for e in range(epochs):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    dev_acc = 0.0\n",
    "    dev_loss = 0.0\n",
    "    for X, Y in train_loader:\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            padding_mask = get_padding_mask(X, word_to_index['<pad>'])\n",
    "            unknown_mask = get_padding_mask(X, word_to_index['<unk>'])\n",
    "            mask = padding_mask+unknown_mask\n",
    "            y_hat = model(X, mask)\n",
    "            loss = loss_fn(y_hat, Y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_acc += metric(y_hat, Y)\n",
    "            train_loss += loss.item()  \n",
    "\n",
    "    # for X, Y in dev_loader:\n",
    "    #     model.eval()\n",
    "    #     opt.zero_grad()\n",
    "    #     X = X.to(device)\n",
    "    #     Y = Y.to(device)\n",
    "    #     with torch.set_grad_enabled(False):\n",
    "    #         mask = get_padding_mask(X, word_to_index['<pad>'])\n",
    "    #         y_hat = model(X, mask)\n",
    "    #         loss = loss_fn(y_hat, Y)\n",
    "    #         dev_acc += metric(y_hat, Y)\n",
    "    #         dev_loss += loss.item()      \n",
    "    # print('[Epoch {} - TRAIN] - Loss: {} - Acc: {} \\n[Epoch {} - DEV]   - Loss: {} - Acc: {}'.format(\n",
    "    #         e,\n",
    "    #         train_loss/t_size,\n",
    "    #         train_acc/t_size, e,\n",
    "    #         dev_loss/d_size, \n",
    "    #         dev_acc/d_size, \n",
    "    #         ))\n",
    "    \n",
    "    print('[Epoch {} - TRAIN] - Loss: {} - Acc: {}'.format(\n",
    "            e,\n",
    "            train_loss/t_size,\n",
    "            train_acc/t_size\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Errors encountered\n",
    "\n",
    "## I. Exploding gradient\n",
    "\n",
    "### a. Explanation\n",
    "When running the training loop with differents parameters, getting issues with CUDA:\n",
    "RuntimeError: CUDA error: device-side assert triggered\n",
    "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1 \n",
    "\n",
    "When putting the optimizer's learning rate to 0.1, This finally end up putting final linear output at values around -1e+23.\n",
    "This resulting in the model outputing \"Nan\" values which cause issue for CUDA or any computations.\n",
    "\n",
    "### b. Resolution \n",
    "\n",
    "When putting LR at 0.01, the problem \"vanish\" and stop happening.\n",
    "\n",
    "### c. Analysis & Other fixes\n",
    "\n",
    "**Hypothesis**: Putting a really high LR at first might \"jump\" and end up in a saddle point where the lowest value being really low, which ultimately end up causing computation issues.\n",
    "\n",
    "This could be verified by putting an LR scheduler and try out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Random Accuracy\n",
    "### a. Explanation\n",
    "\n",
    "After running the model for roughly 100 epochs with parameters:\n",
    "- epochs = 100 \n",
    "- batch_size = 32 \n",
    "- N = 5\n",
    "- heads = 6\n",
    "- optim_lr = 0.01\n",
    "\n",
    "The maximum of training AND dev accuracy (F1 Score) peak at 60%.\n",
    "This basically translate to the model \"guessing\" instead of predicting.\n",
    "\n",
    "Not really sure of the source of the problem right now.\n",
    "\n",
    "**Hypothesis**: Here's differents paths to look on for this:\n",
    "- Transformer model, issue in implementation\n",
    "- Data prep not really good\n",
    "- Not using mask \n",
    "- Parameters not right\n",
    "\n",
    "### b. Resolution \n",
    "\n",
    "Until further Acc or Loss, \"deleting\" the evaluation part of training loop \n",
    "\n",
    "**Data**: For now, I'm putting this issue away to focus on the model only.\n",
    "\n",
    "**Model**: The implementation of the model, after some review, look good.\n",
    "\n",
    "**Mask**: We can start to use masks to for our model to handle and understand the padding tokens\n",
    "By adding it I'm receiving the same F1.\n",
    "Previous call to scaled dot-product was putting mask=None, so the previous F1 score dropping wwas due to other sources.\n",
    "\n",
    "Issue found:\n",
    "In calculating the mask, I've found that the shape of _scores_ tensor look like torch.Size([64, 1, 20, 20]) so with current setup, this could only mean the shape is (batch_size, num_heads, seq_len, seq_len).\n",
    "\n",
    "After reviewing, forgot the matmul and output of it, so nothing to do with seq_len.\n",
    "To apply the mask on this shapes, need to apply two \"unsqueeze\" to add 2 dimensions to match required dimensions and apply the mask.\n",
    "\n",
    "Final mask is made from a padding mask and unknown mask.\n",
    "\n",
    "**Parameters**: When giving higher values for N and heads, the accuracy drop significantly.\n",
    "When putting N=1 and heads=1, the training accuracy now peak at 72% (around 70% constant in dev)\n",
    "Since the task is not really complex, this seems understandable.\n",
    "This is the new base I'm using for now: \n",
    "- epochs = 100\n",
    "- batch_size = 64 \n",
    "- N = 1\n",
    "- heads = 1\n",
    "- optim_lr = 0.01\n",
    "\n",
    "#### Further parameters check:\n",
    "\n",
    "Since we have better results, I mayn't change N and heads values, what I'll check and change on:\n",
    "- Batch size\n",
    "- Learning rate\n",
    "- Optimizer\n",
    "- Loss fn (Since BCE is the most common for binary, I might stick with it)\n",
    "\n",
    "Batch size: After some testing, I'm reaching the top F1 score (0.72) pretty quickly from 2048 as batch size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate\n",
    "\n",
    "As previous problem encountered, it look like our problem start near a saddle point.\n",
    "This might be one of the best way to easily add more Accuracy on our model.\n",
    "\n",
    "**Ways**:\n",
    "- Adding scheduler\n",
    "- Adding momentum to optimizer\n",
    "\n",
    "**Scheduler**:\n",
    "\n",
    "With tested schedulers, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annex\n",
    "\n",
    "For this notebook and creation, I've used multiple sources:\n",
    "- [How to code The Transformer in Pytorch - Toward Data Science - Samuel Lynn-Evans](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec) (As reference for sequence to sequence implementation)\n",
    "- [Attention is All You Need](https://arxiv.org/abs/1706.03762) (As a base)\n",
    "- [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf) (For layer norm)\n",
    "- [ChatGPT](https://openai.com/chatgpt) (For comprehension/question and quick alternatives)\n",
    "- Many Kaggle's Notebooks and Medium/Toward Data Science articles (To add to ChatGPT's response) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
