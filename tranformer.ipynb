{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "This notebook is used to test the classes used and implemented in the transformer_classes.py file\n",
    "\n",
    "Remember:\n",
    "We need to implement the following (found in \"Attention is all you need\" paper available at https://arxiv.org/pdf/1706.03762.pdf):\n",
    "\n",
    "- Transformer Architecture\n",
    "- Scalled Dot Product\n",
    "- Multi-Head Attention\n",
    "\n",
    "And all other things required like positionnal encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Build Vocab from Glove & Embedding\n",
    "\n",
    "First we'll load and setup glove and our vocab to get the first \"brick\" for embedding\n",
    "\n",
    "For this test I'll load all glove pretrained weight and build the vocab\n",
    "\n",
    "#### 1st Version\n",
    "\n",
    "Using Glove and the same data cleaning method as the one in the main branch which let lots of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main/global imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from textfn import *\n",
    "from classes import *\n",
    "from tranformer_classes import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loadDts('dataset/train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 50 # Renammed from embedding_dim\n",
    "glove_path = 'glove_pretrained/glove.6B.{}d.txt'.format(embedding_dim)\n",
    "vocab_size = 10000\n",
    "max_seq_length = 20\n",
    "\n",
    "embeddings = np.zeros((vocab_size+2, embedding_dim))\n",
    "word_to_index = {}\n",
    "index=0\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if index >= vocab_size-2:\n",
    "            break\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float16\")\n",
    "        embeddings[index] = vector\n",
    "        word_to_index[word] = index\n",
    "        index +=1 \n",
    "    f.close()\n",
    "embeddings[index+1] = np.zeros(embedding_dim)\n",
    "embeddings[index+2] = np.zeros(embedding_dim)\n",
    "word_to_index['<unk>'] = index+1\n",
    "word_to_index['<pad>'] = index+2\n",
    "vocab_size+=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TranformerGloveDataset(df, max_seq_length, word_to_index, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. i. Embedding Handling\n",
    "\n",
    "As we built the vocab, we'll need to handle the embedded values of the words.\n",
    "\n",
    "#### Method 1:\n",
    "\n",
    "Using Glove's pretrained weights and freezing the layer in the class Embedder\n",
    "\n",
    "#### Method 2:\n",
    "\n",
    "Idem as method 1 but putting this direclty in the Encoder class using the embedding layer\n",
    "\n",
    "For now, using Method 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Positional Encoding\n",
    "\n",
    "Positional Encoding is a matrix which define the position of the word in the sentence.\n",
    "\n",
    "It's defined with:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "And\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "As stated on the original paper: \n",
    "\"The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed.\"\n",
    "\n",
    "\n",
    "So the dimensions of the PE matrix are the **sentence size** and **embedding size** or $d_{model}$\n",
    "\n",
    "##### Method 1\n",
    "\n",
    "Create a class that compute the positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "# Example to check if positional encoder work\n",
    "data = torch.randn(5, 10, 6)\n",
    "pos_enc = PositionalEncoder(10, 6)\n",
    "encoded_input = pos_enc(data)\n",
    "\n",
    "print(encoded_input.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Multi-Head Attention & Scaled Dot-Product Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
