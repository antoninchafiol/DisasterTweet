{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "This notebook is used to test the classes used and implemented in the transformer_classes.py file\n",
    "\n",
    "Remember:\n",
    "We need to implement the following (found in \"Attention is all you need\" paper available at https://arxiv.org/pdf/1706.03762.pdf):\n",
    "\n",
    "- Transformer Architecture\n",
    "- Scalled Dot Product\n",
    "- Multi-Head Attention\n",
    "\n",
    "And all other things required like positionnal encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function Classes\n",
    "## I. Build Vocab from Glove & Embedding\n",
    "\n",
    "First we'll load and setup glove and our vocab to get the first \"brick\" for embedding\n",
    "\n",
    "For this test I'll load all glove pretrained weight and build the vocab\n",
    "\n",
    "#### 1st Version\n",
    "\n",
    "Using Glove and the same data cleaning method as the one in the main branch which let lots of unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main/global imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from textfn import *\n",
    "from classes import *\n",
    "from tranformer_classes import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loadDts('dataset/train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 50 # Renammed from embedding_dim\n",
    "glove_path = 'glove_pretrained/glove.6B.{}d.txt'.format(d_model)\n",
    "vocab_size = 10000\n",
    "max_seq_length = 20\n",
    "\n",
    "embeddings = np.zeros((vocab_size+2, d_model))\n",
    "word_to_index = {}\n",
    "index=0\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if index >= vocab_size-2:\n",
    "            break\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float16\")\n",
    "        embeddings[index] = vector\n",
    "        word_to_index[word] = index\n",
    "        index +=1 \n",
    "    f.close()\n",
    "embeddings[index+1] = np.zeros(d_model)\n",
    "embeddings[index+2] = np.zeros(d_model)\n",
    "word_to_index['<unk>'] = index+1\n",
    "word_to_index['<pad>'] = index+2\n",
    "vocab_size+=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TranformerGloveDataset(df, max_seq_length, word_to_index, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. i. Embedding Handling\n",
    "\n",
    "As we built the vocab, we'll need to handle the embedded values of the words.\n",
    "\n",
    "#### Method 1:\n",
    "\n",
    "Using Glove's pretrained weights and freezing the layer in the class Embedder\n",
    "\n",
    "#### Method 2:\n",
    "\n",
    "Idem as method 1 but putting this direclty in the Encoder class using the embedding layer\n",
    "\n",
    "For now, using Method 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Positional Encoding\n",
    "\n",
    "Positional Encoding is a matrix which define the position of the word in the sentence.\n",
    "\n",
    "It's defined with:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "And\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "As stated on the original paper: \n",
    "\"The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed.\"\n",
    "\n",
    "\n",
    "So the dimensions of the PE matrix are the **sentence size** and **embedding size** or $d_{model}$\n",
    "\n",
    "##### Method 1\n",
    "\n",
    "Create a class that compute the positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "# Example to check if positional encoder work\n",
    "data = torch.randn(5, 10, 6)\n",
    "pos_enc = PositionalEncoder(10, 6)\n",
    "encoded_input = pos_enc(data)\n",
    "\n",
    "print(encoded_input.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Multi-Head Attention & Scaled Dot-Product Attention \n",
    "\n",
    "Multi-Head Attention is the one of the \"main\" component of the transformer network.\n",
    "\n",
    "It's using a set of matrices which will be trained to handle a specific role in the network:\n",
    "- **Queries (Q)**: Relationship & Dependencies with tokens in sequence.\n",
    "- **Keys (K)**: Key information used to compare against when computing scores.\n",
    "- **Values (V)**: Weighted sum of the mechanism\n",
    "\n",
    "Those matrices are made/initialized from inputs' embeddings vector with the positional encoding.\n",
    "\n",
    "\n",
    "In the Multi-Head Attention, we split the embedding into multiple layers (or **heads**) where $N$ is the number of head. $d_k$ will be refering to the last dimension where $d_k = d_{model}/N$\n",
    "\n",
    "**Dropout**: As the original paper state: \"_We apply dropout to the sums of the embeddings and the positional encoding in both the encoder and decoder stacks. For the base model we use a rate of_ $P_{drop}=0.1$\"\n",
    "\n",
    "### III. i Scaled Dot-Product Attention\n",
    "\n",
    "As the original paper stated, this Attention is computed as: $$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Without forgetting that we can add both mask and dropout to this.\n",
    "\n",
    "#### Method 1 \n",
    "\n",
    "Make the Multi-Head Attention class and the Attention (Scaled Dot-Product Attention) as a function in it.\n",
    "\n",
    "#### Method 2 \n",
    "\n",
    "Idem as method 1, but putting the Attention in a splitted class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Feed-Forward Network\n",
    "\n",
    "The Feed-Forward \"layer\" serve the purpose of deepens the whole networks by using Linear layers.\n",
    "\n",
    "As stated in the original paper: \"_This consists of two linear transformations with a ReLU activation in between_\"\n",
    "\n",
    "The number set per default of $d_{ff}$ is stated in original paper: \"_[...] and the inner-layer has dimensionality_ $d_{ff} = 2048$\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Normalization \n",
    "\n",
    "Normalization is important for our network, to prevent our values to not change too much, so model can train faster and better.\n",
    "\n",
    "Original paper state that they're using Layer Normalization.\n",
    "To implement LN, we need to implement the following:\n",
    "$$LN(z;\\alpha,\\beta) = \\frac{z-\\mu}{\\sigma}\\odot \\alpha + \\beta$$\n",
    "\n",
    "This can be found into the \"Layer Normalization\" paper, Page 13, 15 & 16th formula [HERE](https://arxiv.org/pdf/1607.06450.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Main blocks and Architecture\n",
    "\n",
    "## I. Encoder/Decoder & Problem\n",
    "\n",
    "While both Encoder & Decoder Blocks differ a bit:\n",
    "- Encoder has \"only\" 1 Multi-Head Attention and 1 Feed Forward\n",
    "- Decoder has 2 Multi-Head Attention and 1 Feed Forward and receive Encoder output\n",
    "\n",
    "The common thing between the 2 is the skipped connections and the layers used.\n",
    "So no particular difficulties in implementing the blocks.\n",
    "\n",
    "With all previous classes implemented, we can make the parts classes by adding our embeddings/positional encoding and using either copy.deepcopy() or nn.ModuleList() to can create multiple independent blocks/module for our model to work with.\n",
    "\n",
    "Althought the current transformer is \"finished\", the reference used implemented it for sequence to sequence, but Disaster Tweet is a Sentiment Analysis task, so we need to change a few things to make a sentiment analysis task from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Changes for Sentiment Analysis\n",
    "\n",
    "### i. Transformer class\n",
    "\n",
    "As sentiment analysis is a Many-to-One setup, we don't need the Decoder part of the transformer.\n",
    "So we we'll create a new class for sentiment analysis without decoder, adapt the output, Linear layer and forward computation for a binary output.\n",
    "\n",
    "#### a. Different setups\n",
    "\n",
    "There's different setups related to NLP:\n",
    "- **Many-to-One**: Take a sequence and map it to two or more classes (Ex: Sentiment Analysis)\n",
    "- **Many-to-Many**: Both inputs and output are sequences (Ex: Machine Translation)\n",
    "- **One-to-Many**: Input is a single value and output a sequence (Ex: Image Captioning) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annex\n",
    "\n",
    "For this notebook and creation, I've used multiple sources:\n",
    "- [How to code The Transformer in Pytorch - Toward Data Science - Samuel Lynn-Evans](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec) (As reference for sequence to sequence implementation)\n",
    "- [Attention is All You Need](https://arxiv.org/abs/1706.03762) (As a base)\n",
    "- [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf) (For layer norm)\n",
    "- [ChatGPT](https://openai.com/chatgpt) (For comprehension/question and quick alternatives)\n",
    "- Many Kaggle's Notebooks and Medium/Toward Data Science articles (To add to ChatGPT's response) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
